# Anti Network Pirate - Anomaly Detection Platform (Prototype)
[![Python Script Workflow](https://github.com/tugcantopaloglu/anp/actions/workflows/python-app.yml/badge.svg)](https://github.com/tugcantopaloglu/anp/actions/workflows/python-app.yml)  

This repository contains a prototype implementation of **AnP**, a simplified **Anomaly Detection Platform** developed for research and experimentation purposes. It allows the testing and demonstration of various anomaly detection methods, particularly for time series data such as system resource usage or application logs.

> âš ï¸ **Disclaimer:** This is a prototype project. Several features, models, and modules are developed purely for testing purposes and may be incomplete or unstable.

---

## ğŸš€ Features

- ğŸ“Š Time series anomaly detection
- ğŸ§  Classical ML-based methods (Isolation Forest, One-Class SVM, etc.)
- ğŸ§ª Test cases for memory, CPU, disk, and other system metrics
- ğŸ“ˆ Interactive result visualization using Matplotlib and Pandas
- ğŸ§¼ Synthetic anomaly generation for testing
- ğŸ“ Modular structure for easy experimentation

---

## ğŸ› ï¸ Installation

Clone the repository:

```bash
git clone https://github.com/tugcantopaloglu/anp.git
cd anp
```

Install the required dependencies:

```bash
pip install -r requirements.txt
```

> ğŸ’¡ Note: It is recommended to use a virtual environment.

---

## ğŸ“‚ Project Structure

```
anp/
â”œâ”€â”€ data/                   # Input data files (e.g., system metrics)
â”œâ”€â”€ models/                 # Custom anomaly detection implementations
â”œâ”€â”€ utils/                  # Utility functions (data loading, metrics, etc.)
â”œâ”€â”€ notebooks/              # Sample Jupyter notebooks for testing and visualization
â”œâ”€â”€ main.py                 # Entry point for testing anomaly detection models
â”œâ”€â”€ test_cases/             # Synthetic test case generators
â””â”€â”€ requirements.txt        # Python package requirements
```

---

## ğŸ§ª Usage

Run the main script to execute a test case:

```bash
python main.py --test memory
```

This will:
- Load the corresponding synthetic test data,
- Apply the selected anomaly detection algorithm,
- Visualize results and print anomaly statistics.

Available test types:
- `memory`
- `cpu`
- `disk`
- `multivariate`

---

## ğŸ§  Models

Current anomaly detection methods supported:

- Isolation Forest
- One-Class SVM
- Elliptic Envelope
- Custom thresholding logic

Planned additions:
- LSTM Autoencoders
- Facebook Prophet
- Statistical methods like STL

---

## ğŸ“Š Example Output

Output includes:
- Anomaly scores
- Anomaly flags (binary)
- Visualizations with anomaly points highlighted

You can find example plots and logs inside the `notebooks/` or `outputs/` folders (if enabled).

---

# Model Zoo
| Algorithm | Key Params | Overfitting? | Notes |
|-----------|------------|--------------|-------|
| NaÃ¯ve Bayes | GaussianNB | **Low** | Best overall generalisation. |
| MLP | 2 Ã— 128 neurons, *earlyâ€‘stopping* | Lowâ€‘Medium | 93Â % F1 live. |
| Logistic Reg. | C âˆˆ {1,10} | Medium | 76Â % F1 live â€“ borderline. |
| Random Forest | 300 trees | **High** | Severe overfit; rejected. |
| Decision Tree | depth = None | High | Rejected. |
| SVM | RBF, C = 1 | High | Rejected. |
| KNN | k = 2 | High | Rejected. |
| Gradient Boost | 200 estimators | High | Rejected. |
| LightGBM | default params | High | Rejected. |
| AdaBoost | 200 estimators | High | Rejected. |

## Training & Evaluation
Run **all** experiments:

```bash
python src/train.py --config configs/all_models.yaml
python src/evaluate.py --checkpoint results/model_weights/<model>.pkl
```

Metrics computed:
- **Accuracy**, **Precision**, **Recall**, **F1**, **ROCâ€‘AUC**
- Kâ€‘fold (k = 5) crossâ€‘validation to gauge variance.

Detailed confusion matrices and ROC curves are saved in `results/` for every model.

## Results
| Model | CV Mean | Test Accuracy | Live Accuracy | Live F1 |
|-------|---------|---------------|-----------------------|---------|
| **NaÃ¯ve Bayes** | 0.903 | 0.906 | **0.82** | **0.87** |
| **MLP** | 0.967 | 0.957 | **0.91** | **0.93** |
| Logistic Reg. | 0.957 | 0.954 | 0.74 | 0.76 |
| Others | >Â 0.97 | >Â 0.98 | â‰ˆÂ 0.20 | â‰ˆÂ 0.22 |

**Interpretation**  
Lowâ€‘complexity generative models (NB) and moderately sized neural nets (MLP) balance bias/variance best on heterogeneous traffic, whereas treeâ€‘based ensembles memorise training flows and collapse on unseen subnets.

## Usage
### 1. Batch Scoring
```bash
python src/api.py batch --input data/processed/new_capture.csv                         --model checkpoints/naive_bayes.pkl                         --output predictions.csv
```

### 2. Realâ€‘time API (FastAPI)
```bash
uvicorn src.api:app --host 0.0.0.0 --port 8080
# POST /predict {"features": [...]}  â†’ {"anomaly": true, "score": 0.92}
```

### 3. SIEM Integration
`api.py` returns JSON for direct ingestion by Splunk/ELK.  
A webhook example is provided in `integrations/`.

## Future Work
- **Attack Type Classification** (e.g., DoS vs. Probe) with deep architectures (CNNâ€‘1D, GRU, Transformer).  
- **Online Learning** to adapt to concept drift in evolving traffic.  
- **Adversarial Robustness** testing (evasion & poisoning).  
- **Edge Deployment** on programmable NIC / FPGA for wireâ€‘speed inference.

## ğŸ“Œ TODOs & Notes

- Add advanced anomaly detection models (LSTM, Variational Autoencoders)
- Improve test data generation realism
- Refactor configuration into a YAML-based setup
- Add unit tests for model evaluation and metrics

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

## ğŸ¤ Contributions

Feel free to fork this repository and open pull requests for enhancements, bugfixes, or new models.

---

## ğŸ‘¨â€ğŸ’» Author

Developed by [TuÄŸcan TopaloÄŸlu](https://github.com/tugcantopaloglu)  
For research and educational purposes.
